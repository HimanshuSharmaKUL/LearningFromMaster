{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f573b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee530b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(f'Length of text: {len(text)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c520956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "print(text[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a1d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#before beginning, let us fist see kaun kaunse characters use ho rahe hain\n",
    "chars = set(text) #unique characters choose ho gaye apne aap, unordered\n",
    "chars = sorted(list(set(text))) #list of set -> we get an ordering, an arbitrary ordering tho. Then sorted makes it a particular ordering\n",
    "chars\n",
    "print(''.join(chars))\n",
    "vocab_size = len(chars) #these characters are our vocabulary, we'll make new words etc from these\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a43f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mapping from characters to integers\n",
    "#tokenize: convert the string of chars to some sequence of integers acc to some method\n",
    "\n",
    "#one schema for tokenizing is creating a simple look-up table\n",
    "stoi = {ch:i for i,ch in enumerate(chars)} #string to integer\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6ca067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddccd04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stoi[c] for c in \"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8e6036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos[c] for c in [20, 43, 50, 50, 53]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aec9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "encoder = lambda s: [stoi[c] for c in s] #Takes in s and then does [..] on s \n",
    "decoder = lambda s: [itos[c] for c in s]\n",
    "enc = encoder('Hello')\n",
    "dec = decoder(enc)\n",
    "print(enc)\n",
    "print(''.join(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9e9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#Let us encode and tokenize our dataset\n",
    "data = encoder(text)\n",
    "import torch \n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3e2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train adn validation test\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ced728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We do not feed the whole training data into the neural networks, we work with chunks of the data\n",
    "#we sample random chunks, some maximum length, aka block size\n",
    "#we call this chunks as context window, or block size\n",
    "block_size = 8\n",
    "train_data[:block_size+1] #here are 9 characters, with 8 examples to train on\n",
    "#18 ke bad 47 aata hai\n",
    "#18 & 47 ke bad 56\n",
    "#18,47,56, ke bad 57 etc and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58de3348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] #block size characters\n",
    "y = train_data[1:block_size+1] #next block_size characters, since it is offset by 1\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #input is the chars upto and including t\n",
    "    target = y[t] #target is the t-th character in target array y\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d819e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is tensor([24]) the target: 43\n",
      "when input is tensor([24, 43]) the target: 58\n",
      "when input is tensor([24, 43, 58]) the target: 5\n",
      "when input is tensor([24, 43, 58,  5]) the target: 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target: 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target: 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target: 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target: 39\n",
      "when input is tensor([44]) the target: 53\n",
      "when input is tensor([44, 53]) the target: 56\n",
      "when input is tensor([44, 53, 56]) the target: 1\n",
      "when input is tensor([44, 53, 56,  1]) the target: 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target: 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target: 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target: 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is tensor([52]) the target: 58\n",
      "when input is tensor([52, 58]) the target: 1\n",
      "when input is tensor([52, 58,  1]) the target: 58\n",
      "when input is tensor([52, 58,  1, 58]) the target: 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target: 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target: 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target: 46\n",
      "when input is tensor([25]) the target: 17\n",
      "when input is tensor([25, 17]) the target: 27\n",
      "when input is tensor([25, 17, 27]) the target: 10\n",
      "when input is tensor([25, 17, 27, 10]) the target: 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target: 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target: 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target: 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "#creating batches of the data so that the GPUs are busy and GPUs can train them independently\n",
    "batch_size = 4 #number of indepndent sequences to be trained in parallel every fwd and backward passs of the transformer\n",
    "block_size = 8 #what is max context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x an y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) #smple a Random location in the whole dataset, pure dataset (from 0 to len(data)-blocksize) me se ek random sample lena hai, 'batch_size'=4 jitne random nos.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #.stack(list, dim=0 default) - stack them in rows of 4 rows (batch size) x 8 size ka tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "# ix, _, _ = get_batch('train')\n",
    "# [data[i:i+block_size] for i in ix]\n",
    "# [data[i] for i in ix]\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension or block/context window dimention\n",
    "        context = xb[b, :t+1] #b-th batch item lo, us itme ke 0 se t+1 tk bloack chars lo. Input comes from x array\n",
    "        target = yb[b, t] #bth batch item lo, aur us itme ka sirf tth char lo. Target comes from y\n",
    "        print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695493a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8 65\n",
      "torch.Size([32, 65])\n",
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Now we feed the train data to the simplest language model neural network - i.e. bigram model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) \n",
    "        #we create a token embedding table of vocabsize x vocabsize\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C)\n",
    "        #when we pass an index, i.e. xb of size 4x8, then \n",
    "        #every single integer of our xb will refer to the embedding table\n",
    "        #and plucks out a row of that embedding table corresponding to its index\n",
    "        #ex: we have \n",
    "        # xb = > torch.Size([4, 8])\n",
    "        # tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "        #         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "        #         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "        #         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "        #so, 24 will pluck out 24th row, \n",
    "        # 43 will pluck out 43rd row and so on\n",
    "        #and then pytorch arranges them into B,T,C - batch, time, channel tensor\n",
    "        #batch = 4, time = 8, channel = 65 (vocab size), C is also the 'classes'. Basically, the classes into which we're classifying\n",
    "        #and we interpret them as logits, ie scores for next character in the sequence\n",
    "        \n",
    "        #-ve log liklihood loss aka cross entropy\n",
    "        \n",
    "        B,T,C = logits.shape\n",
    "        print(B,T,C)\n",
    "        logits = logits.view(B*T, C) #pytorch wants 'C' as 2nd dimension, so we squash/combine the first 2 dims into one dim and make C as 2nd dim\n",
    "        print(logits.shape)\n",
    "        targets = targets.view(B*T) \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of the B no. of current context-s (of length T)\n",
    "        #job of generate is to extend the (B,T) to (B, T+1) to (B, T+2) and so on\n",
    "        #i.e. for all rows in a Batch (i.e. B =4) the columns inc from T to T+1, T+1 is then fed back to generate (T+1)+1 = T+2 and this goes on till the number of max_tookens that we want\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes (B,C)\n",
    "            #apply softmax to get probablities\n",
    "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
    "            #append sampled indec to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb) #forward() is called -> forward(self, xb, yb) .'. idx = xb \n",
    "print(logits.shape) #out.shape\n",
    "print(loss) #-log(1/65) ~ 4.17\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a2c76",
   "metadata": {},
   "source": [
    "‚úÖ What are logits?\n",
    "In machine learning, particularly classification tasks:\n",
    "\n",
    "Logits are the raw, unnormalized scores output by a model before applying a probability function like softmax.\n",
    "\n",
    "They are real numbers (positive or negative) representing how \"likely\" the model thinks each class is ‚Äî but not yet turned into probabilities.\n",
    "\n",
    "For example, if the model outputs logits [2.0, 1.0, 0.1], applying softmax will turn them into something like [0.65, 0.24, 0.11], i.e., valid probabilities that sum to 1.\n",
    "\n",
    "üîç What is this line doing?\n",
    "```python\n",
    "logits = self.token_embedding_table(idx)\n",
    "```\n",
    "Let‚Äôs break it down:\n",
    "`self.token_embedding_table` is an instance of:\n",
    "```python\n",
    "nn.Embedding(vocab_size, vocab_size)\n",
    "```\n",
    "So `self.token_embedding_table` is a learnable table of shape (vocab_size, vocab_size). Think of it as a matrix:\n",
    "\n",
    "```css\n",
    "[ token_0_vector ]   ‚Üê vocab_size rows\n",
    "[ token_1_vector ]      each of size vocab_size\n",
    "[ ...          ]\n",
    "```\n",
    "`idx` is a tensor of shape `(B, T)` where:\n",
    "`B` is batch size,\n",
    "`T` is sequence length,\n",
    "each value in `idx` is an integer token index from the vocabulary.\n",
    "\n",
    "When you do:\n",
    "```python\n",
    "self.token_embedding_table(idx)\n",
    "```\n",
    "You are looking up the embedding vector for each token in `idx`.\n",
    "Output shape: `(B, T, vocab_size)`\n",
    "\n",
    "ü§î Why are we calling the output \"logits\"?\n",
    "Here‚Äôs the key idea:\n",
    "This model is very simple ‚Äî it doesn't transform embeddings further. Each token index is directly mapped to a vector of size vocab_size, which is treated as the raw scores (logits) for predicting the next token.\n",
    "So the vector you get for each token isn't just a general embedding ‚Äî it‚Äôs interpreted as the logits for the next token prediction.\n",
    "In short:\n",
    "Normally, models have:\n",
    "```java\n",
    "token ‚Üí embedding ‚Üí neural layers ‚Üí output logits\n",
    "```\n",
    "But here, we skip the middle:\n",
    "```java\n",
    "token ‚Üí output logits (via embedding directly)\n",
    "```\n",
    "This is why it's a bigram model ‚Äî it predicts the next token based only on the current token, without any context from earlier tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1eb464",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding This Line of Code\n",
    "\n",
    "```python\n",
    "logits = self.token_embedding_table(idx)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ What‚Äôs the Goal?\n",
    "\n",
    "Understand what this line does when:\n",
    "\n",
    "- `idx.shape = (4, 8)` ‚Äî a tensor of token IDs (batch of 4 sequences, each of length 8)\n",
    "- `self.token_embedding_table = nn.Embedding(65, 65)` ‚Äî a learnable embedding table\n",
    "- 65 rows (one for each token in the vocabulary),\n",
    "- 65 columns (each row is a 65-dimensional vector = size of output).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What is `nn.Embedding`?\n",
    "\n",
    "`nn.Embedding(num_embeddings, embedding_dim)` is a lookup table:\n",
    "It works like a dictionary:\n",
    "For every token index `i ‚àà [0, vocab_size - 1]`, it returns a vector of size `embedding_dim`.\n",
    "- Input: token ID (integer from 0 to 64)\n",
    "- Output: a learnable vector of length 65\n",
    "\n",
    "```python\n",
    "nn.Embedding(65, 65)  # 65 tokens, each mapped to a 65-dimensional vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ What Happens in `self.token_embedding_table(idx)`? What happens when you pass in a `(4, 8)` tensor?\n",
    "\n",
    "- `idx` has shape `(4, 8)` ‚Üí token indices\n",
    "- Output has shape `(4, 8, 65)` ‚Üí each token index replaced by its 65-d vector\n",
    "\n",
    "PyTorch applies the embedding lookup element-wise across the tensor.\n",
    "\n",
    "```python\n",
    "output[b, t] = embedding_table[idx[b, t]]\n",
    "output.shape = (4,8,65) #output\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Visualization\n",
    "\n",
    "#### Input:\n",
    "```text\n",
    "idx (4x8):\n",
    "[[4, 21, 7, 15,  ...],\n",
    " [9, 14, 23, 3,  ...],\n",
    " ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### Embedding Table (65x65):\n",
    "```text\n",
    "[\n",
    " [0.1, -0.4, ...,  0.6],   ‚Üê token 0\n",
    " [0.0,  0.2, ..., -0.9],   ‚Üê token 1\n",
    " ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```text\n",
    "logits = embedding_table[idx] ‚Üí shape (4, 8, 65)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Why Are They Called ‚ÄúLogits‚Äù?\n",
    "\n",
    "Each 65-dimensional output vector at `(b, t)` represents:\n",
    "\n",
    "> ‚ÄúGiven token `idx[b, t]`, here are scores for what the **next token** might be.‚Äù\n",
    "\n",
    "These are **raw scores** (logits), not probabilities. You apply softmax during training or inference to get probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Concept | Meaning |\n",
    "|--------|---------|\n",
    "| `idx.shape` | `(4, 8)` ‚Äî token IDs |\n",
    "| `token_embedding_table` | `nn.Embedding(65, 65)` ‚Äî lookup table |\n",
    "| Output shape | `(4, 8, 65)` ‚Äî each token mapped to 65-d logits |\n",
    "| Why \"logits\"? | They‚Äôre used to predict the next token (via softmax + cross-entropy) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
