{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f573b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee530b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(f'Length of text: {len(text)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c520956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "print(text[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a1d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#before beginning, let us fist see kaun kaunse characters use ho rahe hain\n",
    "chars = set(text) #unique characters choose ho gaye apne aap, unordered\n",
    "chars = sorted(list(set(text))) #list of set -> we get an ordering, an arbitrary ordering tho. Then sorted makes it a particular ordering\n",
    "chars\n",
    "print(''.join(chars))\n",
    "vocab_size = len(chars) #these characters are our vocabulary, we'll make new words etc from these\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a43f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mapping from characters to integers\n",
    "#tokenize: convert the string of chars to some sequence of integers acc to some method\n",
    "\n",
    "#one schema for tokenizing is creating a simple look-up table\n",
    "stoi = {ch:i for i,ch in enumerate(chars)} #string to integer\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6ca067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddccd04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stoi[c] for c in \"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8e6036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos[c] for c in [20, 43, 50, 50, 53]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aec9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "encoder = lambda s: [stoi[c] for c in s] #Takes in s and then does [..] on s \n",
    "decoder = lambda s: [itos[c] for c in s]\n",
    "enc = encoder('Hello')\n",
    "dec = decoder(enc)\n",
    "print(enc)\n",
    "print(''.join(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9e9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#Let us encode and tokenize our dataset\n",
    "data = encoder(text)\n",
    "import torch \n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3e2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train adn validation test\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ced728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We do not feed the whole training data into the neural networks, we work with chunks of the data\n",
    "#we sample random chunks, some maximum length, aka block size\n",
    "#we call this chunks as context window, or block size\n",
    "block_size = 8\n",
    "train_data[:block_size+1] #here are 9 characters, with 8 examples to train on\n",
    "#18 ke bad 47 aata hai\n",
    "#18 & 47 ke bad 56\n",
    "#18,47,56, ke bad 57 etc and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58de3348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] #block size characters\n",
    "y = train_data[1:block_size+1] #next block_size characters, since it is offset by 1\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #input is the chars upto and including t\n",
    "    target = y[t] #target is the t-th character in target array y\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d819e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is tensor([24]) the target: 43\n",
      "when input is tensor([24, 43]) the target: 58\n",
      "when input is tensor([24, 43, 58]) the target: 5\n",
      "when input is tensor([24, 43, 58,  5]) the target: 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target: 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target: 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target: 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target: 39\n",
      "when input is tensor([44]) the target: 53\n",
      "when input is tensor([44, 53]) the target: 56\n",
      "when input is tensor([44, 53, 56]) the target: 1\n",
      "when input is tensor([44, 53, 56,  1]) the target: 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target: 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target: 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target: 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is tensor([52]) the target: 58\n",
      "when input is tensor([52, 58]) the target: 1\n",
      "when input is tensor([52, 58,  1]) the target: 58\n",
      "when input is tensor([52, 58,  1, 58]) the target: 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target: 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target: 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target: 46\n",
      "when input is tensor([25]) the target: 17\n",
      "when input is tensor([25, 17]) the target: 27\n",
      "when input is tensor([25, 17, 27]) the target: 10\n",
      "when input is tensor([25, 17, 27, 10]) the target: 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target: 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target: 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target: 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "#creating batches of the data so that the GPUs are busy and GPUs can train them independently\n",
    "batch_size = 4 #number of indepndent sequences to be trained in parallel every fwd and backward passs of the transformer\n",
    "block_size = 8 #what is max context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x an y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) #smple a Random location in the whole dataset, pure dataset (from 0 to len(data)-blocksize) me se ek random sample lena hai, 'batch_size'=4 jitne random nos.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #.stack(list, dim=0 default) - stack them in rows of 4 rows (batch size) x 8 size ka tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "# ix, _, _ = get_batch('train')\n",
    "# [data[i:i+block_size] for i in ix]\n",
    "# [data[i] for i in ix]\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension or block/context window dimention\n",
    "        context = xb[b, :t+1] #b-th batch item lo, us itme ke 0 se t+1 tk bloack chars lo. Input comes from x array\n",
    "        target = yb[b, t] #bth batch item lo, aur us itme ka sirf tth char lo. Target comes from y\n",
    "        print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695493a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8 65\n",
      "torch.Size([32, 65])\n",
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Now we feed the train data to the simplest language model neural network - i.e. bigram model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) \n",
    "        #we create a token embedding table of vocabsize x vocabsize\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C)\n",
    "        #when we pass an index, i.e. xb of size 4x8, then \n",
    "        #every single integer of our xb will refer to the embedding table\n",
    "        #and plucks out a row of that embedding table corresponding to its index\n",
    "        #ex: we have \n",
    "        # xb = > torch.Size([4, 8])\n",
    "        # tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "        #         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "        #         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "        #         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "        #so, 24 will pluck out 24th row, \n",
    "        # 43 will pluck out 43rd row and so on\n",
    "        #and then pytorch arranges them into B,T,C - batch, time, channel tensor\n",
    "        #batch = 4, time = 8, channel = 65 (vocab size), C is also the 'classes'. Basically, the classes into which we're classifying\n",
    "        #and we interpret them as logits, ie scores for next character in the sequence\n",
    "        \n",
    "        #-ve log liklihood loss aka cross entropy\n",
    "        \n",
    "        B,T,C = logits.shape\n",
    "        print(B,T,C)\n",
    "        logits = logits.view(B*T, C) #pytorch wants 'C' as 2nd dimension, so we squash/combine the first 2 dims into one dim and make C as 2nd dim\n",
    "        print(logits.shape)\n",
    "        targets = targets.view(B*T) \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of the B no. of current context-s (of length T)\n",
    "        #job of generate is to extend the (B,T) to (B, T+1) to (B, T+2) and so on\n",
    "        #i.e. for all rows in a Batch (i.e. B =4) the columns inc from T to T+1, T+1 is then fed back to generate (T+1)+1 = T+2 and this goes on till the number of max_tookens that we want\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes (B,C)\n",
    "            #apply softmax to get probablities\n",
    "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
    "            #append sampled indec to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb) #forward() is called -> forward(self, xb, yb) .'. idx = xb \n",
    "print(logits.shape) #out.shape\n",
    "print(loss) #-log(1/65) ~ 4.17\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a2c76",
   "metadata": {},
   "source": [
    "✅ What are logits?\n",
    "In machine learning, particularly classification tasks:\n",
    "\n",
    "Logits are the raw, unnormalized scores output by a model before applying a probability function like softmax.\n",
    "\n",
    "They are real numbers (positive or negative) representing how \"likely\" the model thinks each class is — but not yet turned into probabilities.\n",
    "\n",
    "For example, if the model outputs logits [2.0, 1.0, 0.1], applying softmax will turn them into something like [0.65, 0.24, 0.11], i.e., valid probabilities that sum to 1.\n",
    "\n",
    "🔍 What is this line doing?\n",
    "```python\n",
    "logits = self.token_embedding_table(idx)\n",
    "```\n",
    "Let’s break it down:\n",
    "`self.token_embedding_table` is an instance of:\n",
    "```python\n",
    "nn.Embedding(vocab_size, vocab_size)\n",
    "```\n",
    "So `self.token_embedding_table` is a learnable table of shape (vocab_size, vocab_size). Think of it as a matrix:\n",
    "\n",
    "```css\n",
    "[ token_0_vector ]   ← vocab_size rows\n",
    "[ token_1_vector ]      each of size vocab_size\n",
    "[ ...          ]\n",
    "```\n",
    "`idx` is a tensor of shape `(B, T)` where:\n",
    "`B` is batch size,\n",
    "`T` is sequence length,\n",
    "each value in `idx` is an integer token index from the vocabulary.\n",
    "\n",
    "When you do:\n",
    "```python\n",
    "self.token_embedding_table(idx)\n",
    "```\n",
    "You are looking up the embedding vector for each token in `idx`.\n",
    "Output shape: `(B, T, vocab_size)`\n",
    "\n",
    "🤔 Why are we calling the output \"logits\"?\n",
    "Here’s the key idea:\n",
    "This model is very simple — it doesn't transform embeddings further. Each token index is directly mapped to a vector of size vocab_size, which is treated as the raw scores (logits) for predicting the next token.\n",
    "So the vector you get for each token isn't just a general embedding — it’s interpreted as the logits for the next token prediction.\n",
    "In short:\n",
    "Normally, models have:\n",
    "```java\n",
    "token → embedding → neural layers → output logits\n",
    "```\n",
    "But here, we skip the middle:\n",
    "```java\n",
    "token → output logits (via embedding directly)\n",
    "```\n",
    "This is why it's a bigram model — it predicts the next token based only on the current token, without any context from earlier tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1eb464",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding This Line of Code\n",
    "\n",
    "```python\n",
    "logits = self.token_embedding_table(idx)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 What’s the Goal?\n",
    "\n",
    "Understand what this line does when:\n",
    "\n",
    "- `idx.shape = (4, 8)` — a tensor of token IDs (batch of 4 sequences, each of length 8)\n",
    "- `self.token_embedding_table = nn.Embedding(65, 65)` — a learnable embedding table\n",
    "- 65 rows (one for each token in the vocabulary),\n",
    "- 65 columns (each row is a 65-dimensional vector = size of output).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What is `nn.Embedding`?\n",
    "\n",
    "`nn.Embedding(num_embeddings, embedding_dim)` is a lookup table:\n",
    "It works like a dictionary:\n",
    "For every token index `i ∈ [0, vocab_size - 1]`, it returns a vector of size `embedding_dim`.\n",
    "- Input: token ID (integer from 0 to 64)\n",
    "- Output: a learnable vector of length 65\n",
    "\n",
    "```python\n",
    "nn.Embedding(65, 65)  # 65 tokens, each mapped to a 65-dimensional vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 What Happens in `self.token_embedding_table(idx)`? What happens when you pass in a `(4, 8)` tensor?\n",
    "\n",
    "- `idx` has shape `(4, 8)` → token indices\n",
    "- Output has shape `(4, 8, 65)` → each token index replaced by its 65-d vector\n",
    "\n",
    "PyTorch applies the embedding lookup element-wise across the tensor.\n",
    "\n",
    "```python\n",
    "output[b, t] = embedding_table[idx[b, t]]\n",
    "output.shape = (4,8,65) #output\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Visualization\n",
    "\n",
    "#### Input:\n",
    "```text\n",
    "idx (4x8):\n",
    "[[4, 21, 7, 15,  ...],\n",
    " [9, 14, 23, 3,  ...],\n",
    " ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### Embedding Table (65x65):\n",
    "```text\n",
    "[\n",
    " [0.1, -0.4, ...,  0.6],   ← token 0\n",
    " [0.0,  0.2, ..., -0.9],   ← token 1\n",
    " ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```text\n",
    "logits = embedding_table[idx] → shape (4, 8, 65)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Why Are They Called “Logits”?\n",
    "\n",
    "Each 65-dimensional output vector at `(b, t)` represents:\n",
    "\n",
    "> “Given token `idx[b, t]`, here are scores for what the **next token** might be.”\n",
    "\n",
    "These are **raw scores** (logits), not probabilities. You apply softmax during training or inference to get probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Concept | Meaning |\n",
    "|--------|---------|\n",
    "| `idx.shape` | `(4, 8)` — token IDs |\n",
    "| `token_embedding_table` | `nn.Embedding(65, 65)` — lookup table |\n",
    "| Output shape | `(4, 8, 65)` — each token mapped to 65-d logits |\n",
    "| Why \"logits\"? | They’re used to predict the next token (via softmax + cross-entropy) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
